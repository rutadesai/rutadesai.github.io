
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
/* Stolen from Sergey */
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 14px
  }
  strong {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 13px
  }
  heading {
        font-family: 'Lato', Verdana, Helvetica, sans-serif;
        font-size: 15px;
        font-weight: 700
  }
  #container {
    width: 80%;
}
  </style>
  <link rel="icon" href="https://www.cs.cmu.edu/sites/all/themes/scs2017/favicon.ico">

  <title>Ruta Desai</title>
  
  <link href="./aayushb_wfiles/css" rel="stylesheet" type="text/css">
</head>

<body>
  <table width="1100" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td width="67%" valign="middle">

              <p align="center"><font size="6">Ruta Desai</font>

</p><p style = "text-align:justify">I am a Staff Research Scientist in <a href="https://ai.meta.com/">Fundamental AI Research (FAIR)</a> at Meta. At FAIR, I am exploring reasoning and long-horizon planning problems towards agents that can partner with humans</a>. Prior to that, I was a Tech Lead Manager at <A href="https://tech.fb.com/ar-vr/">Meta Reality Labs Research</A>. At Reality Labs, I led an embodied AI team focused on solving perception and planning problems for Augmented Reality (AR) systems that can extend their users' capabilities. I am broadly interested in perception, planning, and reasoning problems for Human-AI interaction. 
<!-- We adopt an embodied AI framing for contextual AR assistance. In particular, we model the AR system as an intelligent agent that perceives the user and their environment through various sensors and outputs actions in service of the user's goals, aligned with the user's preferences.  -->
 <br>
<p> I obtained my PhD at the <A href="http://www.ri.cmu.edu/">Robotics Institute</A>, <A href="http://www.cmu.edu"> Carnegie Mellon University</A> in 2018 working with <A href="http://www.cs.cmu.edu/~scoros/">Stelian Coros</a> and <A href="https://www.cs.cmu.edu/~jmccann/">Jim McCann</a>. 
For my thesis, I devised human-AI systems that enabled casual users to build and program robots &#8212 towards increasing accessibility of robotics. In the past, I have also had not-so-brief stints in the area of human motor control and biomechanical simulations with <A href="http://www.cs.cmu.edu/~hgeyer/">Hartmut Geyer</a> and <A href="http://www.cs.cmu.edu/~jkh/">Jessica K. Hodgins</A>. </p>
<br>

<!-- <b>Please get in touch if you're interested in applying for an internship at FAIR.</b> -->

</p>

          </td>
          <td width="20%"><img width="200" src="images/ruta_circle.png"><br><p align=center>Email: rutadesai[at]meta.com</p><p align=center>
          <a href="./CV_Ruta.pdf">CV</a> &nbsp/&nbsp
          <a href="https://ai.facebook.com/people/ruta-desai/">Meta AI</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?hl=en&user=bwZFR4EAAAAJ">Scholar</a><br>
          </tr>
        </tbody></table>

  <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        
      <tbody><tr><td>
        <h1>Research Highlights</h1>
        <h2> Embodied AI for AR </h2>
<p style = "text-align:justify"> My current research focuses on leveraging representation learning, planning, and reinforcement learning techniques &#8212 combined with an embodied AI framing &#8212 for developing AR assistance models. Our embodied AI framing allows us to break down the problem of computing AR/VR assistance into two main parts &#8212 a) Inferring the goals and context of the user given a sequence of multi-modal sensor observations and b) Learning goal-conditioned system-action policies.</p> <!-- with different form factors and functionalities.</p> -->
    

<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <td width="25%"><div id="container">
<iframe width="400" height="225" src="https://www.youtube.com/embed/NMTer_2DMfg?autoplay=1&mute=1&rel=0&loop=1&playlist=NMTer_2DMfg" allowfullscreen ></iframe>
</div>
    </td><td width="75%" valign="top">
      <p><b>Goal and Context Inference for AR Assistance</b><br>
<br>
<p style = "text-align:justify"> We leverage self-supervised learning and multi-modal sensors (egocentric video + IMU) to understand user's goals and context such as current task and actions in AR.</p><br>
<em> ICCV EPIC Workshop 2021 <br> 
        <p>
<br>
    </td>
  </tr>

</td></tr>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <td width="25%"><div id="container"> 
<iframe width="400" height="225" src="https://www.youtube.com/embed/U2GprdlQ61k?autoplay=1&mute=1&rel=0&loop=1&playlist=U2GprdlQ61k" allowfullscreen></iframe> <!-- 350X200 old size-->
<!-- </div>
    </td><td width="75%" valign="top">
      <p><b>Towards Goal-Conditioned Assistance in VR</b><br>
<br>
<p style = "text-align:justify">We compute assistance policies for house-cleaning task in VR and evaluate it at-scale by deploying these policies in a web-based version of the AI Habitat simulator.</p><br>
<em> <a href="https://arxiv.org/abs/2010.07358">Preprint</a> available! </em>
        <p>
<br>
    </td>
  </tr>

</td></tr>
</table> -->

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <td width="25%"><div id="container">
<img src="images/emqa.png" alt="" width="400" style="border-style: none">
</div>
    </td><td width="75%" valign="top">
      <p><b>Episodic Memory Question Answering for AR Assistance </b><br>
<br>
<p style = "text-align:justify"> Can we develop egocentic AI models that can respond to a user's queries about their spatiotemporal history?</p><br>
<br>
 <em> CVPR 2022 </em><br>
 <em> In collaboration with <a href="https://www.gatech.edu/">Georgia Tech</a></em>
        <p>
<br>
    </td>
  </tr>

</td></tr>
</table> --> 


        <!-- <h2> Human-AI Systems for Robot Design </h2>
        <p style = "text-align:justify"> My PhD research aimed at making robotics more accessible to casual users by reducing the domain knowledge required in designing and building robots. Towards this end, I developed several interactive human-in-the-loop AI systems that enable the design of desired structure and behavior of diverse robots.</p> <!-- with different form factors and functionalities.</p> -->
            
        
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><div id="container">
  <iframe width="400" height="225" src="https://www.youtube.com/embed/9tq_ymFeHKc?autoplay=1&mute=1&rel=0&loop=1&playlist=9tq_ymFeHKc" allowfullscreen ></iframe>
</div>
            </td><td width="75%" valign="top">
              <p><b>Interactive AI System for Articulated Robot Design</b><br>
        <br>
        <p style = "text-align:justify"> This tool allows novices to create custom articulated robots such as manipulators and walking robots. It supports both manual and automatic design, and enables design testing using physics-based simulation.</p><br>
        <em> ICRA 2017 <br> 
		<br>Featured on <a href="https://techcrunch.com/2017/06/01/new-toolkit-makes-it-easy-to-drag-and-drop-your-own-robot/">Techcrunch</a> and <a href="https://cacm.acm.org/news/218246-robot-design-for-dummies/fulltext">ACM</a>! </em><br>
        <em> Try it <a href="./files/articulated_robot_design.zip">out</a>!
                <p>
        <br>
            </td>
          </tr>

    </td></tr>
  
  
              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><div id="container"> 
  <iframe width="400" height="225" src="https://www.youtube.com/embed/U1CViN8OECs?autoplay=1&mute=1&rel=0&loop=1&playlist=U1CViN8OECs" allowfullscreen></iframe> <!-- 350X200 old size-->
<!-- </div>
            </td><td width="75%" valign="top">
              <p><b>Interactive AI System for Non-articulated Robot Design</b><br>
        <br>
        <p style = "text-align:justify"> 
        This tool enables novices to create smart IoT devices with embedded sensors using digital fabrication. It automatically finds assembly-aware packing of components within the device, and exports necessary geometries for 3D printing.</p><br>
        <em> UIST 2018 </em>
                <p>
        <br>
            </td>
          </tr>

    </td></tr>
  </table> --> 
  
                <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><div id="container">
      <img src="images/semanticTeaser2.gif" alt="" width="400" height="225" style="border-style: none">
</div>
            </td><td width="75%" valign="top">
              <p><b>Semantic Design of Expressive Robot Behaviors</b><br>
        <br>
        <p style = "text-align:justify"> Can we design complex robot behaviors such as robot walking based on the emotion that the behavior evokes?</p><br>
        <br>
         <em> CHI 2019 </em><br><br>
        <em> <A href="https://chi2019.acm.org/2019/03/15/chi-2019-best-papers-honourable-mentions/">Best paper award</A>!<br>  
        <em> In collaboration with <a href="https://www.autodeskresearch.com/groups/user-interface">Autodesk Research, Toronto</a></em>
                <p>
        <br>
            </td>
          </tr>

    </td></tr>
  </table> -->
  
  <!-- </tbody></table> -->

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr><td>
      <h1>Publications</h1>
    <h2>Digital and Physical Agents</h2>
    </tr></td>
 </tbody></table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/askact2.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Ask-to-Act: Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning</b>
<br>Ram Ramrakhya, Matthew Chang, Xavier Puig, <b>Ruta Desai</b>, Zsolt Kira, and Roozbeh Mottaghi<br><br>
<em> [Under Review] International Conference on Learning Representations (ICLR), Brazil (2026). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br><a href="https://arxiv.org/pdf/2504.00907">PDF</a> 
<br>
</td>
</tr> </tbody>
</table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/coral.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Collaborative Reasoner: Self-improving Social Agents with Synthetic Conversations</b>
<br>Ansong Ni*, <b>Ruta Desai*</b>, Yang Li, Xinjie Lei, Dong Wang, Jane Yu, Ramya Raghavendra, Gargi Gosh, Daniel Li, Asli Celikyilmaz<br><br>
<em> Advances in Neural Information Processing Systems (Neurips), San Diego, USA (2025). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br><a href="https://scontent-sea1-1.xx.fbcdn.net/v/t39.2365-6/491349761_1434003347954852_6520604589974594081_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TnadIiSHIDEQ7kNvwHjWmbt&_nc_oc=AdkKj4JoLnGjxgN5LbW8WXI_M1eQzPw9ugf6jMvNY7MAdP17HEOwjGLE1F6xa8j-BBU&_nc_zt=14&_nc_ht=scontent-sea1-1.xx&_nc_gid=pSXjzng1da2PbcM5BjukBA&oh=00_AfMxGUcPwJvc1w1j9p2jHTRWztVDKuH07Z0kV80Tnoo-5A&oe=68651DE5">PDF</a> | <a href="https://github.com/facebookresearch/collaborative-reasoner">Code</a> | <a href="https://ai.meta.com/blog/meta-fair-updates-perception-localization-reasoning/">Meta AI Blog</a>
<br>
</td>
</tr> </tbody>
</table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/coopera.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>COOPERA: Continual Open-Ended Human-Robot Assistance</b>
<br>Chenyang Ma, Kai Lu, <b>Ruta Desai*</b>, Xavier Puig*, Andrew Markham*, Niki Trigoni* <br><br>
<em> Advances in Neural Information Processing Systems (Neurips), San Diego, USA (2025). <b>Spotlight: Top 3% paper</b> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br>PDF Coming soon!  
<br>
</td>
</tr> </tbody>
</table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/adapt.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>ADAPT: Actively Discovering and Adapting to Preferences for any Task</b>
<br>Maithili Patel, Xavier Puig, <b>Ruta Desai</b>, Roozbeh Mottaghi, Sonia Chernova, Joanne Truong, and Akshara Rai<br><br>
<em> Conference on Language Modeling (COLM), Montreal, Canada (2025). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br><a href="https://arxiv.org/pdf/2504.04040">PDF</a> 
<br>
</td>
</tr> </tbody>
</table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/partnr.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks</b>
<br>Matthew Chang, Gunjan Chhablani, Alexander Clegg, Mikael Dallaire Cote, <b>Ruta Desai</b>, Michal Hlavac, Vladimir Karashchuk, Jacob Krantz, Roozbeh Mottaghi, Priyam Parashar, Siddharth Patki, Ishita Prasad, Xavier Puig, Akshara Rai, Ram Ramrakhya, Daniel Tran, Joanne Truong, John M. Turner, Eric Undersander, Tsung-Yen Yang<br><br>
<em> International Conference on Learning Representations (ICLR), Singapore (2025). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br><a href="https://arxiv.org/pdf/2411.00081">PDF</a> | <a href="https://github.com/facebookresearch/partnr-planner/tree/main/">Code</a> | <a href="https://aihabitat.org/partnr/">Webpage</a> | <a href="https://ai.meta.com/blog/machine-intelligence-research-new-models/">Meta AI Blog</a>
<br>
</td>
</tr> </tbody>
</table>
<!--
 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/iros24.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Learning Human Preferences Over Robot Behavior as Soft Planning Constraints</b>
<br>Austin Narcomey, Nathan Tsoi, <b>Ruta Desai</b>, and Marynel Vazquez<br><br>
<br><a href="https://arxiv.org/abs/2403.19795">arXiv Preprint</a> 
<br>
</td>
</tr> </tbody>
</table>
-->

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/h3.gif" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots</b>
<br>Xavi Puig*, Eric Undersander*, Andrew Szot*, Mikael Cote*, Ruslan Partsey*, Jimmy Yang*, <b>Ruta Desai*</b>, Alexander Clegg*, Michal Hlavac, Tiffany Min, Theo Gervet, Vladimir Vondrus, Vincent-Pierre Berges, John Turner, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai**, Roozbeh Mottaghi**<br><br>
<em> International Conference on Learning Representations (ICLR), Vienna, Austria (2024). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em>
<br><a href="https://arxiv.org/pdf/2310.13724.pdf">PDF</a> | <a href="https://github.com/facebookresearch/habitat-lab/tree/v0.3.0">Code</a> | <a href="https://aihabitat.org/habitat3/">Webpage</a>
<br>
</td>
</tr> </tbody>
</table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/zsc.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Adaptive Coordination in Social Embodied Rearrangement</b>
<br>Andrew Szot, Unnat Jain, Dhruv Batra, Zsolt Kira, <b>Ruta Desai</b>, and Akshara Rai<br><br>
<em> International Conference on Machine Learning (ICML), Hawaii, USA (2023). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br><a href="https://arxiv.org/pdf/2306.00087.pdf">PDF</a> | <a href="https://github.com/facebookresearch/habitat-lab/tree/social-eai#readme">Code</a>
<br>
</td>
</tr> </tbody>
</table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/morp.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Effective Baselines for Multiple Object Rearrangement Planning in Partially Observable Mapped Environments</b>
<br>Engin Tekin, Elaheh Barati, Nitin Kamra, and <b>Ruta Desai</b> <br><br>
<em> AAAI Workshop on User-Centric AI for Assistance in At-Home Tasks, Washington DC, USA (2023). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br><br>
<a href="https://arxiv.org/pdf/2301.09854.pdf">arXiv</a> | <a href="./cite/morp.bib">bib</a> 
</td>
</tr> </tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/semIL2.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Cross-Domain Imitation Learning via Semantic Skills</b>
<br>Karl Pertsch, <b>Ruta Desai</b>, Franziska Meier, Vikash Kumar, Dhruv Batra, and Akshara Rai <br><br>
<em> Conference on Robot Learning(CoRL), Aukland, New Zealand (2022). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br><a href="https://arxiv.org/pdf/2212.07407.pdf">PDF</a> | <a href="./cite/star.bib">bib</a> | <A href="https://www.youtube.com/watch?v=ghme5dX-49o&t=2s">Video</A> | <a href="https://kpertsch.github.io/star/">Webpage</a>
<br>
</td>
</tr> </tbody>
</table>
 
 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody><tr><td>
   <h2>Vision and Language</h2>
   </tr></td>
</tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/wacv26.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction</b>
<br>Ce Zhang, Yale Song, <b>Ruta Desai</b>, Michael Louis Iuzzolino, Joseph Tighe, Gedas Bertasius, and Satwik Kottur <br><br>
<em> [Under Review] IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), Arizona, USA. (2026). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em>
<br><a href="https://arxiv.org/pdf/2507.15130">PDF</a> 
</td>
</tr> </tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/wacv.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b> User-in-the-loop Evaluation of Multimodal LLMs for Activity Assistance</b>
<br>Mrinal Verghese*, Brian Chen*, Hamid Eghbalzadeh, Tushar Nagarajan, and <b>Ruta Desai</b> <br><br>
<em> IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), Arizona, USA. (2025). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em>
<br><a href="https://www.arxiv.org/abs/2408.03160">PDF</a> | <a href="https://youtu.be/00grUlGa0Kk">Video</a>
</td>
</tr> </tbody>
</table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/vlamp.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b> Pretrained Language Models as Visual Planners for Human Assistance</b>
<br>Dhruvesh Patel, Hamid Engbalzadeh, Nitin Kamra, Michael Louis Iuzzolino, Unnat Jain, and <b>Ruta Desai</b> <br><br>
<em> International Conference on Computer Vision (ICCV), Paris, France (2023). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br><a href="https://arxiv.org/pdf/2304.09179.pdf">PDF</a> | <a href="https://github.com/facebookresearch/vlamp">Code</a> 
</td>
</tr> </tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/egotv.gif" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b> EgoTV: Egocentric Task Verification from Natural Language Task Descriptions </b>
<br>Rishi Hazra, Brian Chen, Akshara Rai, Nitin Kamra, and <b>Ruta Desai</b> <br><br>
<em> International Conference on Computer Vision (ICCV), Paris, France (2023). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br><a href="https://arxiv.org/pdf/2303.16975.pdf">PDF</a> | <a href="https://github.com/facebookresearch/EgoTV">Code</a> | <a href = "https://rishihazra.github.io/EgoTV/">Webpage</a> 
</td>
</tr> </tbody>
</table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/adtg3.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Action Dynamics Task Graphs for Learning Plannable Representations of Procedural Tasks</b>
<br>Weichao Mao, <b>Ruta Desai</b>, Michael Louis Iuzzolino, and Nitin Kamra <br><br>
<em> AAAI Workshop on User-Centric AI for Assistance in At-Home Tasks, Washington DC, USA (2023). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br><a href="https://arxiv.org/abs/2302.05330">arXiv</a> | <a href="./cite/adtg23.bib">bib</a> 
</td>
</tr> </tbody>
</table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/envsim2real.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Egocentric Scene Context for Human-centric Environment Understanding from Video</b>
<br>Tushar Nagarajan, Santhosh Kumar Ramakrishnan, <b>Ruta Desai</b>, James Hillis, and Kristen Grauman <br><br>
<em> Advances in Neural Information Processing Systems (Neurips), New Orleans, USA (2023). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em>
<br><a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/bd2605c5d854837aaf095537e82f1883-Paper-Conference.pdf">PDF</a> | <a href="./cite/sim2real.bib">bib</a> | <a href="https://vision.cs.utexas.edu/projects/ego-env/">Webpage</a>
</td>
</tr> </tbody>
</table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/emqa2.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Episodic Memory Question Answering</b>
<br>Samyak Datta, Sameer Dharur, Vincent Cartellier, <b>Ruta Desai</b>, Mukul Khanna, Dhruv Batra, and Devi Parikh <br><br>
<em> The Conference on Computer Vision and Pattern Recognition (CVPR), New Orleans, USA (2022). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em>
<br><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Datta_Episodic_Memory_Question_Answering_CVPR_2022_paper.pdf">PDF</a> | <a href="./cite/emqa.bib">bib</a> | <a href ="https://www.youtube.com/watch?v=_K5CPq8kuRE">Video</a>
</td>
</tr> </tbody>
</table>

 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/SSL4.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>How You Move Your Head Tells What You Do: Self-supervised Video Representation Learning with Egocentric Cameras and IMU Sensors</b>
<br>Satoshi Tsutsui, <b>Ruta Desai</b>, and Karl Ridgeway<br><br>
<em> International Conference on Computer Vision (ICCV), EPIC Workshop (2021). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br><a href="https://arxiv.org/pdf/2110.01680.pdf">PDF</A> | <a href="./cite/iccv21.bib">bib</a> | <A href="https://www.youtube.com/watch?v=NMTer_2DMfg">Video</A> 
<br>
</td>
</tr> </tbody>
</table>


   <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr><td>
     <h2>Human-AI Interaction</h2>
     </tr></td>
  </tbody></table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<td width="28%"><img src="images/chi26.png" alt="" width="325" style="border-style: none">
</td><td width="64%" valign="top">
  <p><b>Interactive Program Synthesis for Modeling Collaborative Physical Activities from Narrated Demonstrations</b>
<br>Edward Kim, Daniel He, Jorge Chao, Wiktor Rajca, Mohammed Amin, Nishant Malpani, <b>Ruta Desai</b>, Antti Oulasvirta, Bjoern Hartmann, and Sanjit Seshia<br><br>
<em> [Under Review]  ACM Conference on Human Factors in Computing Systems (CHI), Spain (2026). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br><a href="https://arxiv.org/pdf/2509.24250?">PDF</a> 
<br>
</td>
</tr> </tbody>
</table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <td width="28%"><img src="images/yichi.png" alt="" width="325" style="border-style: none">
    </td><td width="64%" valign="top">
      <p><b>A Meta-Bayesian Approach for Rapid Online Parametric Optimization for Wrist-based Interactions</b>
  <br>Yi-Chi Liao, <b>Ruta Desai</b>, Alec M Pierce, Krista E Taylor, Hrvoje Benko, Tanya R Jonker, Aakar Gupta <br><br>
  <em> ACM Conference on Human Factors in Computing Systems (CHI), Hawaii, USA (2024). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em>
  <br><a href="https://dl.acm.org/doi/pdf/10.1145/3613904.3642071">PDF</a> | <a href="https://www.youtube.com/watch?v=pxLGCKIEhjc&ab_channel=ACMSIGCHI">Video</a> 
  <br>
  </td>
  </tr> </tbody>
  </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/uist22.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Optimizing the Timing of Intelligent Suggestion in Virtual Reality</b>
<br>Difeng Yu, <b>Ruta Desai</b>, Ting Zhang, Hrvoje Benko, Tanya R. Jonker, and Aakar Gupta <br><br>
<em> ACM User Interface Software and Technology Symposium (UIST), Bend, USA (2022). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em>
<br><a href="http://www.hbenko.com/publications/2022/Yu_OptimizingTiming_UIST2022.pdf">PDF</a> | <a href="./cite/wheat.bib">bib</a> | <a href="https://www.youtube.com/watch?v=Q4sFc-WJx4g&ab_channel=ACMSIGCHI">Video</a>
<br>
</td>
</tr> </tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/kondo2.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Optimal Assistance for Object-Rearrangement Tasks in Augmented Reality</b>
<br>Benjamin Newman, Kevin Carlberg, and <b>Ruta Desai</b><br><br>
<em> arXiv (2020). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<br><a href="https://arxiv.org/pdf/2010.07358.pdf">PDF</A> | <a href="./cite/kondo.bib">bib</a> | <A href="https://www.youtube.com/watch?v=2v3li5GyNaY">Video</A> 
<br>
</td>
</tr> </tbody>
</table>

     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
           <td width="28%"><img src="images/spotit.png" alt="" width="325" style="border-style: none">
           </td><td width="64%" valign="top">
             <p><b>	Towards Inferring Cognitive State Changes from Pupil Size Variations in Real World Conditions</b>
 <br>Naga Venkata Kartheek Medathati, <b>Ruta Desai</b>, and James Hillis<br><br>
 <em> ACM Symposium on Eye Tracking Research and Applications (ETRA), Stuttgart, Germany (2020). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
 <br><a href="https://github.com/rutadesai/rutadesai.github.io/blob/master/files/etra2020.pdf">PDF</A> | <a href="./cite/etra2020.bib">bib</a> 
   <br>
   </td>
   </tr> </tbody>
   </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="28%"><img src="images/chi19.jpg" alt="" width="325" style="border-style: none">
            </td><td width="64%" valign="top">
              <p><b>Geppetto: Enabling Semantic Design of Expressive Robot Behaviors</b>
	<br><b>Ruta Desai</b>, Fraser Anderson, Justin Matejka, Stelian Coros, James McCann, George Fitzmaurice, and Tovi Grossman<br><br>
	<em> ACM Conference on Human Factors in Computing Systems (CHI), Glasgow, UK (2019). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
<em> <font color="red"> Best paper award (top 1%) </font> <A href="https://chi2019.acm.org/2019/03/15/chi-2019-best-papers-honourable-mentions/">[Details]</A> </em>
	<br><a href="https://github.com/rutadesai/rutadesai.github.io/blob/master/files/chi2019.pdf">PDF</A> | <a href="./cite/chi2019.bib">bib</a> | <A href="https://youtu.be/DXbnwodJ2Ks">Video</A> | <A href="https://youtu.be/1I1Bsa6V0L8">Fastforward</A> | <A href="./files/chi2019_supp.pdf">Supplementary (PDF)</A>
    <br>
    </td>
    </tr> </tbody>
</table>
  
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/uist18.jpg" alt="" width="325" style="border-style: none">
            </td><td width="64%" valign="top">
              <p><b>Assembly-aware Design of Printable Electromechanical Devices</b>
	<br><b>Ruta Desai</b>, James McCann, and Stelian Coros<br><br>
	<em> ACM User Interface Software and Technology Symposium (UIST), Berlin, Germany (2018). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;</em>
	<br><a href="./files/uist18.pdf">PDF</A> | <a href="./cite/uist18.bib">bib</a> | <A href="https://youtu.be/7gsYSpJfIa4">Video</A> | <A href = "https://youtu.be/niu-XPIItao"> Fastforward </A>
    <br>
    </td>
    </tr> </tbody>
</table>
  
    			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/siggraph.png" alt="" width="325" style="border-style: none">
            </td><td width="64%" valign="top">
              <p><b>Skaterbots: Optimization-based Design and Motion Synthesis for Robotic Creatures with Legs and Wheels</b>
<br> Moritz Geilinger, Roi Poranne, <b>Ruta Desai</b>, Bernhard Thomaszewski, and Stelian Coros<br><br>
 <em> ACM Transactions on Graphics (Proc. ACM SIGGRAPH), Vancouver, Canada (2018). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;</em>
 <br><a href="http://crl.ethz.ch/papers/Skaterbots.pdf">PDF</A> | <a href="./cite/sigg18.bib">bib</a> | <A href="http://crl.ethz.ch/videos/skaterbots.mp4">Video</A>
        <br>
            </td>
          </tr> </tbody>
</table>

  			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/icraworkshop.png" alt="" width="325" style="border-style: none">
            </td><td width="64%" valign="top">
              <p><b>Automatic Design of Task-specific Robotic Arms</b>
<br> <b>Ruta Desai</b>, Margarita Safonova, Katharina Muelling, and Stelian Coros<br><br>
 <em> ICRA Workshop on Autonomous Robot Design, Brisbane, Australia (2018). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;</em><br>
 <br><a href="https://arxiv.org/abs/1806.07419">arXiv</A> | <a href="./cite/icra2018wkshp.bib">bib</a> | <a href="./files/2018_ICRA_slides.pdf">Slides</a> | <A href="https://youtu.be/MOrijZu47EA">Video</A>
        <br>
            </td>
          </tr> </tbody>
</table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/icra18.png" alt="" width="325" style="border-style: none">
            </td><td width="64%" valign="top">
              <p><b>Interactive Co-Design of Form and Function for Legged Robots using the Adjoint Method</b>
<br> <b>Ruta Desai</b>, Beichen Li, Ye Yuan, and Stelian Coros<br><br>
 <em> International Conference on Climbing and Walking Robots (CLAWAR), Panama city, Panama (2018). &nbsp;</em><br>
 <br><em> <font color="red"> Best technical paper (second place)  &nbsp; </font> </em><br>
 <a href="https://arxiv.org/abs/1801.00385v2">arXiv Preprint</A> | <A href="https://www.youtube.com/watch?v=zrMZBgTbJho&feature=youtu.be">Video</A>
        <br>
            </td>
          </tr> </tbody>
</table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/icra17.png" alt="" width="325" style="border-style: none">
            </td><td width="64%" valign="top">
              <p><b>Computational Abstractions for Interactive Design of Robotic Devices</b>
<br> <b>Ruta Desai</b>, Ye Yuan, and Stelian Coros<br><br>
 <em> IEEE International Conference on Robotics and Automation (ICRA), Singapore (2017).&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;</em>
 <br><a href="./files/2017_ICRA_Ruta.pdf">PDF</A> | <a href="./cite/icra2017.bib">bib</a> | <a href="./files/2017_ICRA_slides.pdf">Slides</a> | <A href="https://www.youtube.com/watch?v=PGpTsQtznw4">Video</A>
 | <a href="./files/articulated_robot_design.zip">Code</a><br>
 Robot models for fabrication: <a href="https://www.myminifactory.com/object/robo-calligrapher-45664">car</a>, <a href="https://www.myminifactory.com/object/puppy-45663"> walking robot </a><br>
                <p>
        <br>
            </td>
          </tr>

  </tbody>
</table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/chi.jpg" alt="" width="325" style="border-style: none">
            </td><td width="64%" valign="top">
 <b>3D Printing Pneumatic Device Controls with Variable Activation Force Capabilities</b>
<br> Marynel Vazquez, Eric Brockmeyer, <b>Ruta Desai</b>, Chris Harrison and Scott E. Hudson<br><br>
 <em> ACM Conference on Human Factors in Computing Systems (CHI), Seoul, Korea, (2015).</em><br>
<br><A href="https://www.ri.cmu.edu/pub_files/2015/4/3D-Printing-Pneumatic-Device-Controls-with-Variable-Activation-Force-Capabilities-Paper.pdf">PDF</A> | <a href="./cite/chi.bib">bib</a> | <a href="./files/3DPrintingCHI.pdf">Poster</a> | <a href="https://www.youtube.com/watch?v=CoIepxH4Pow"> Video </a>  
<br>
    <p>
        <br>
            </td>
          </tr>

</tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr><td>
          <h1></h1>
      <h2>Human Motion Modeling, Human Motor Control, Biomechanics</h2>
      </tr></td>
    </tbody></table>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/DW_v2.jpg" alt="" width="325" style="border-style: none">
            </td><td width="64%" valign="top">
 <b>A Simple Model of Skill Acquisition in a Dynamic Balance Task</b>
<br> <b>Ruta Desai</b> and Jessica K. Hodgins<br><br>
 <em> Dynamic Walking, Columbus, USA (2015).</em><br>
<br><A href="http://www.ri.cmu.edu/pub_files/2015/7/DW15Abstract_DesaiRuta.pdf">PDF</A> | <a href="./cite/dw.bib"> bib</a> | <a href="./files/2015_DW_poster.pdf">Poster</a> | <a href="https://www.youtube.com/watch?v=u_3KUYrdAFI"> Video</a>
            </td>
          </tr>
    
</tbody>  
</table>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/humanoids_v1.jpg" alt="" width="325" style="border-style: none">
            </td><td width="64%" valign="top">
          <b>Virtual Model Control for Dynamic Lateral Balance</b>
<br> <b>Ruta Desai</b>, Hartmut Geyer, and Jessica K. Hodgins<br><br>
 <em> IEEE International Conference on Humanoid Robots (Humanoids), Madrid, Spain (2014).</em><br>
 <br><A href="http://www.ri.cmu.edu/pub_files/2014/11/07041464.pdf">PDF</A> | <a href="./cite/humanoids.bib">bib</a>
  <br>
            </td>
          </tr>
    
</tbody>  
</table>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/NMModel.jpg" alt="" width="325" style="border-style: none">
            </td><td width="64%" valign="top">
<b>Integration of an Adaptive Swing Control into a Neuromuscular Human Walking Model</b>
<br> Seungmoon Song, <b>Ruta Desai</b>, and Hartmut Geyer<br><br>
 <em> IEEE Engineering in Medicine and Biology Society (EMBC), Osaka, Japan (2013).</em><br>
<br><A href="http://www.ri.cmu.edu/pub_files/2013/7/SongEA13bIEEE_EMBC.pdf">PDF</A> | <a href="./cite/embc.bib">bib</a> | <a href="https://www.youtube.com/watch?v=mGFyqXEOLgE&feature=youtu.be"> Video </a>
<br>

  <br>
            </td>
          </tr>
    
</tbody>
</table>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/icra.jpg" alt="" width="325" style="border-style: none">
            </td><td width="64%" valign="top">
<b>Muscle-Reflex Control of Robust Swing Leg Placement</b>
<br> <b>Ruta Desai</b> and Hartmut Geyer<br><br>
 <em> IEEE International Conference on Robotics and Automation (ICRA), Karlsruhe, Germany (2013).</em><br>
<br> <A href="http://www.ri.cmu.edu/pub_files/2013/5/Desai-Geyer13IEEE_ICRA.pdf">PDF</A> | <a href="./cite/icra.bib">bib</a>
<br>
  <br>
            </td>
          </tr>
    
</tbody>
</table>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/robio_v5.jpg" alt="" width="325" style="border-style: none">
            </td><td width="75%" valign="top">
<b>Robust Swing Leg Placement under Large Disturbances</b>
<br><b>Ruta Desai</b> and Hartmut Geyer<br><br>
 <em> IEEE International Conference on Robotics and Biomimetics (ROBIO), Guangzhou, China (2012).</em><br>
<br> <A href="http://www.ri.cmu.edu/pub_files/2012/12/Desai-Geyer12IEEEROBIO.pdf">PDF</A> | <a href="./cite/robio.bib">bib</a> | <a href ="https://www.cs.cmu.edu/~hgeyer/Videos/SWING_Demo.mp4"> Video </a>
<br>
  <br>
</td></tr></tbody>
</table>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr><td>
    <h1></h1>
      <h2>Selected Patents and Preprints</h2>
      </td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/survey3.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Human Action Anticipation: A Survey</b>
<br>Bolin Lai*, Sam Toyer*, Tushar Nagarajan, Rohit Girdhar, Shengxin Zha, James M. Rehg, Kris Kitani, Kristen Grauman, <b>Ruta Desai</b>, and Miao Liu <br> <br>
<em> <a href="https://arxiv.org/pdf/2410.14045">arXiv Preprint</a> (2024). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
</td>
</tr> </tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="28%"><img src="images/lifeplanner.png" alt="" width="325" style="border-style: none">
  </td><td width="64%" valign="top">
    <p><b>Human-Centered Planning</b>
<br>Yuliang Li, Nitin Kamra, <b>Ruta Desai</b>, and Alon Halevy <br> <br>
<em> <a href="https://arxiv.org/pdf/2311.04403.pdf">arXiv Preprint</a> (2023). &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;</em><br>
</td>
</tr> </tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="25%"><img src="images/kondo_patent.png" alt="" width="325" style="border-style: none">
  </td><td width="75%" valign="top">
<b>Optimal Assistance for Object-Rearrangement Tasks in Augmented Reality</b>
<br> Benjamin A. Newman, Kevin T. Carlberg, <b>Ruta Desai</b>, James Hillis<br>
<br> <em> <A href="https://patents.google.com/patent/US20220114366A1/en">US Patent US20220114366 A1</A> (2022).</em>
<br>
<br>
  </td>
</tr>
</table> 

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <td width="25%"><img src="images/patent_gepetto2.png" alt="" width="325" style="border-style: none">
  </td><td width="75%" valign="top">
<b>Generative design techniques for robot behavior</b>
<br> Fraser Anderson, Stelian Coros, <b>Ruta Desai</b>, Tovi Grossman, Justin Matejka, George Fitzmaurice <br> <br>
<em> <A href="https://patents.google.com/patent/US20200030988A1/en">US Patent 20200030988 A1</A> (2020).&nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</em><br>
<br>
<br>
  </td>
</tr>
</table> 

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/patent_v4.png" alt="" width="325" style="border-style: none">
            </td><td width="75%" valign="top">
<b>Robust swing leg controller under large disturbances</b>
<br> <b>Ruta Desai</b> and Hartmut Geyer <br><br>
 <em> <A href="https://www.google.com/patents/US20150066156">US Patent 20150066156 A1</A> (2014).&nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</em><br>
<br>
  <br>
            </td>
          </tr>
</table> 
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/CMU_RI_small.jpg" alt="" width="320" style="border-style: none">
            </td><td width="64%" valign="top">
<b>A Brief Overview of Human and Robot Motor Learning</b>
<br> <b>Ruta Desai</b><br><br>
 <em><A href="https://www.ri.cmu.edu/pub_files/2015/10/MotorLearningReview_RutaDesai.pdf">RI CMU Technical Report</A></em>&nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</em><br>
            </td>
          </tr>
    
</tbody>
</table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">      
  <tbody><tr><td>
       <h1>Academic Service</h1>
     <h2>Conference Program Committee Chair and Reviewer </h2>
    <ul>
    <li> Program Committee, <a href="https://aaai.org/conference/aaai/aaai-25/">AAAI</a> (2025).</li>
    <li> Organizer, <a href="https://corrworkshop.github.io/">CVPR Workshop</a> on Causal and Object-centric Representations for Robotics (2024).</li>
    <li> Reveiwer for IEEE CVPR, IEEE ICCV, ICLR, Neurips, ICML (2023-2024).</li>
    <li> Program committee, ACM UIST (<a href="http://uist.acm.org/uist2019/organizers/">2019</a>-<a href="https://uist.acm.org/uist2020/organizers.html">2020</a>). </li>
    <li> Associate Chair (AC), ACM CHI (<a href="https://chi2020.acm.org/">2020</a>-<a href="https://chi2021.acm.org/organising/organising-committee">2021</a>).</li>
    <li> Reveiwer for IEEE IROS, IEEE ICRA, ACM TOCHI, ACM UIST, ACM CHI, ACM GI, IEEE WHC (2015-2022).</li>
    </ul>
        </tr></td> 
      <tr><td>
      <h2>Talks and Teaching</h2>
    <ul>
    <li>Co-Instructor for <a href ="https://courses.cs.washington.edu/courses/cse455/25sp/">Computer Vision</a>, University of Washington, 2025. </li>
    <li>Guest Lecture in <a href ="https://faculty.eng.ufl.edu/jain/teaching/generating-expressiveness-in-intelligent-agents-and-avatars/">Generating Expressiveness in Intelligent Agents and Avatars</a>, University of Florida, 2022. </li>
    <li> <A href="https://dub.washington.edu/seminars/2020-02-26.html">DUB Seminar</A>, University of Washington, 2020.</li>
    <li><A href= "http://schedule.bid-seminar.com/speakers/175">BID Seminar</A>, University of California, Berkeley, 2019.</li>
    <li> <A href="https://www.grasp.upenn.edu/events/ruta-desai">GRASP Seminar</A>, University of Pennsylvania, 2019.</p>
   </li></ul>
    </td></tr> 
     </tbody></table>
   
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">      
  <tbody><tr><td>
       <h1>Outreach</h1>
     <h2>Women in Technology</h2>
    <p>I am always looking to support and contribute towards encouraging women in science and technology. In the past: <br>
    <ul>
    <li align="left">I have led <A href="https://docs.google.com/presentation/d/1Ssw25HqdhB-IWfo5fsKjOaXwrBLcD3udgS0vo9ZvLRw/edit#slide=id.p26">roadshows</A> and volunteered in teaching middle school girls at <a href = "https://www.cmu.edu/scs/technights/">Technights</a> at CMU. We have smashed computers, fought <a href ="https://docs.google.com/presentation/d/1d-ZjcquBKfSi5QI6NoGo8DwArB2WgYryyxZJFya8aSE/edit#slide=id.p">combats in code</a>, and have even gazed stars through apps!</li>
    <li align="left"> I helped organize <a href="https://www.cmu.edu/cs/ourcs/index.html">OurCS</a> 2015, a conference to encourage undergraduate women in research. I was also a part of Google Anita Borg Scholars <a href = "https://www.facebook.com/pages/Anita-Borg-Scholarship-Alumni-Community/261363020730608">Alumni Planning committee</a>.
    </li>
    </ul>
        </tr></td> 
      <!-- <tr><td>
      <b>CMU Laptop Rehab</b>
      <br>
    <p align="left">A bunch of us refurbish old laptops and desktops and donate them to schools. <A href="https://sites.google.com/site/cmulaptoprehab/">Read more!</A></p>
        <br>
    </td></tr>  -->
     </tbody></table>

     <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">      
      <tbody><tr><td>
           <h1>Selected Press</h1>
        <ul>
          <li align="left"> <A href="https://siliconangle.com/2023/10/20/metas-habitat-3-0-simulates-real-world-environments-intelligent-ai-robot-training/">Silicon Angle</A>, Meta's Habitat 3.0 simulates real-world environments for intelligent AI robot training, 2023.
          <li align="left"> <A href="https://techcrunch.com/2023/10/20/embodied-ai-spins-a-pen-and-helps-clean-the-living-room-in-new-research/">Techcrunch</A>, Embodied AI spins a pen and helps clean the living room in new research, 2023.
          <li align="left"> <A href="https://techcrunch.com/2017/06/01/new-toolkit-makes-it-easy-to-drag-and-drop-your-own-robot/">Techcrunch</A>, New toolkit makes it easy to drag and drop your own robot, 2017.
          <li align="left"> <A href="https://cacm.acm.org/news/218246-robot-design-for-dummies/fulltext">ACM Communications</A>, Robot Design For Dummies, 2017.
          <li align="left"> <A href="https://www.eurekalert.org/pub_releases/2017-05/cmu-cit053017.php">EurekAlert</A>, CMU's interactive tool helps novices and experts make custom robots, 2017.
          <li align="left"> <A href="http://erc-assoc.org/content/graduate-students-earn-prestigious-scholarships-women-anca-dragan-and-ruta-desai">NSF</A>, Graduate Student Earns Prestigious Scholarships for Women - Ruta Desai, 2012.
          <li align="left"> <A href="https://www.cs.cmu.edu/news/five-scs-students-named-siebel-scholars">CMU SCS</A>, Five SCS Students Named Siebel Scholars, 2012.
        </li>
        </ul>
            </tr></td> 
          <!-- <tr><td>
          <b>CMU Laptop Rehab</b>
          <br>
        <p align="left">A bunch of us refurbish old laptops and desktops and donate them to schools. <A href="https://sites.google.com/site/cmulaptoprehab/">Read more!</A></p>
            <br>
        </td></tr>  -->
         </tbody></table>

  <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        
  <tbody><tr><td>
       <h1>Miscellaneous Projects</h1>
               <p style = "text-align:justify"> Some fun projects from the past.</p> 
     </tr></td>
  </tbody></table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="25%"><img src="images/BO_NN.png" alt="" width="375 style="border-style: none">
            </td><td width="64%" valign="center">
              <p><b>Bayesian Optimization with a Neural Network Kernel</b>
<br> Akshara Rai, <b>Ruta Desai</b>, and Siddharth Gopal<br>
 <br><a href="./files/BO_NN.pdf">PDF</A> 
        <br>
            </td>
          </tr> </tbody>
</table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <td width="15%"><img src="images/fountainFig.png" alt="" width="375 style="border-style: none">
            </td><td width="64%" valign="center">
              <p><b>Personalized Fountains that Create 3D Shapes Out of Water</b>
<br> <b>Ruta Desai</b> and Arun Srivatsan Rangaprasad<br>
 <br><a href="./files/fountain.pdf">PDF</A>  | <a href = "https://www.youtube.com/watch?v=s4gPyIxHUZY"> Video</a> (duck-shaped fountain)
                <p>
        <br>
            </td>
          </tr>

  </tbody>
</table> -->

<script type="text/javascript">


var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
        document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script><script src="./aayushb_wfiles/ga.js" type="text/javascript"></script> <script type="text/javascript">
try {
        var pageTracker = _gat._getTracker("UA-39749944-1");
        pageTracker._trackPageview();
        } catch(err) {}
        </script>
      </td>
    </tr>
  </tbody></table>

<br>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>
        <p align="right"><font size="2">
    Last updated: October 2025
        <p align="right"><font size="2">
Webpage adapted from <a href="http://www.cs.berkeley.edu/~barron/">Jon Barron.</a>
</font></p>

</td></tr>
</table>

      </td>
    </tr>
  </table>


</body></html>
